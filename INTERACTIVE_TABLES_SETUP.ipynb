{
 "metadata": {
  "kernelspec": {
   "display_name": "Snowflake",
   "language": "sql",
   "name": "snowflake"
  },
  "language_info": {
   "name": "sql"
  },
  "lastEditStatus": {
   "notebookId": "qrnf25scyeyjuh3aqdmi",
   "authorId": "432459825768",
   "authorName": "BMHESS",
   "authorEmail": "brian.hess@snowflake.com",
   "sessionId": "8312fc20-30bd-45e9-89e3-e90396622eba",
   "lastEditTime": 1768594861881
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Intro",
    "collapsed": false
   },
   "source": "# Snowflake Setup for Kafka Interactive Tables Demo\n\nThis notebook contains **all the SQL commands** needed to set up Snowflake for the Kafka Interactive Tables Quickstart. \nThis notebook is the companion notebook for the [Real-Time Analytics with Kafka, Interactive Tables, and Snowpipe Streaming v2 Quickstart](https://www.snowflake.com/en/developers/guides/kafka-interactive-tables-streaming/)\n\n## What This Notebook Does\n\n1. **Set Context** - Creates database and schema\n2. **Create Role and User** - Sets up Kafka connector authentication\n3. **Key Pair Authentication** - Configures RSA public key\n4. **Interactive Table Creation** - Creates an Interactive Table optimized for sensor data\n5. **Interactive Warehouse Setup** - Creates and configures an Interactive Warehouse\n6. **Grant Permissions** - Grants ownership to Kafka connector role\n7. **Sample Queries** - Provides queries for analyzing streaming data\n8. **Cleanup** - Commands to remove all demo resources\n\n## Prerequisites\n- ACCOUNTADMIN role access\n- RSA key pair generated (see instructions below)",
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Part1_SetContext"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 1: Set Context\n",
    "\n",
    "Create the database and schema for the demo."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___SetContext"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\n\n-- Create database and schema if they don't exist\nCREATE DATABASE IF NOT EXISTS KAFKA_INTERACTIVE;\nCREATE SCHEMA IF NOT EXISTS KAFKA_INTERACTIVE.STREAMING;",
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "id": "eefc7e35-7962-4514-98a5-6cd6c1cef282",
   "metadata": {
    "language": "sql",
    "name": "___SetContext2"
   },
   "outputs": [],
   "source": "-- Set context\nUSE DATABASE KAFKA_INTERACTIVE;\nUSE SCHEMA STREAMING;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Part2_CreateRoleAndUser"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 2: Create Role and User for Kafka Connector\n",
    "\n",
    "The Kafka connector needs its own role with specific permissions to:\n",
    "- Use the database and schema\n",
    "- Create and manage tables\n",
    "- Create stages and pipes for streaming"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___CreateRole"
   },
   "outputs": [],
   "source": "-- Create a role for the Kafka connector\nCREATE ROLE IF NOT EXISTS KAFKA_CONNECTOR_ROLE;\nGRANT ROLE KAFKA_CONNECTOR_ROLE TO ROLE ACCOUNTADMIN;\n\n-- Grant necessary privileges\nGRANT USAGE ON DATABASE KAFKA_INTERACTIVE TO ROLE KAFKA_CONNECTOR_ROLE;\nGRANT USAGE ON SCHEMA KAFKA_INTERACTIVE.STREAMING TO ROLE KAFKA_CONNECTOR_ROLE;\nGRANT CREATE TABLE ON SCHEMA KAFKA_INTERACTIVE.STREAMING TO ROLE KAFKA_CONNECTOR_ROLE;\nGRANT CREATE STAGE ON SCHEMA KAFKA_INTERACTIVE.STREAMING TO ROLE KAFKA_CONNECTOR_ROLE;\nGRANT CREATE PIPE ON SCHEMA KAFKA_INTERACTIVE.STREAMING TO ROLE KAFKA_CONNECTOR_ROLE;",
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___CreateUser"
   },
   "outputs": [],
   "source": [
    "-- Create a user for the Kafka connector\n",
    "CREATE USER IF NOT EXISTS KAFKA_USER\n",
    "  DEFAULT_ROLE = KAFKA_CONNECTOR_ROLE\n",
    "  DEFAULT_NAMESPACE = KAFKA_INTERACTIVE.STREAMING;\n",
    "\n",
    "GRANT ROLE KAFKA_CONNECTOR_ROLE TO USER KAFKA_USER;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Part3_ConfigureKeyPairAuth",
    "collapsed": false
   },
   "source": "---\n\n# Part 3: Configure Key Pair Authentication\n\nThe Kafka connector uses RSA key pair authentication for secure access to Snowflake.\n\n## Generate RSA Key Pair (Run in Terminal)\n\nBefore running the next cell, generate an RSA key pair in your terminal:\n\n```bash\n# Generate private key (unencrypted for simplicity in this demo)\nopenssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt\n\n# Generate public key\nopenssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub\n\n# Display the public key (you'll need this for the next step)\ncat rsa_key.pub | grep -v \"BEGIN\\|END\" | tr -d '\\n'\n```\n\nCopy the public key content (without the `-----BEGIN PUBLIC KEY-----` and `-----END PUBLIC KEY-----` lines).",
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "AssignKey"
   },
   "source": [
    "## Step 3.1: Assign RSA Public Key to User\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:** Replace the placeholder below with your actual public key content (on a single line, no headers/footers)."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___AssignKey"
   },
   "outputs": [],
   "source": "-- Replace YOUR_PUBLIC_KEY_HERE with your actual public key content\n-- Example: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8A...\nALTER USER KAFKA_USER SET RSA_PUBLIC_KEY='YOUR_PUBLIC_KEY_HERE';",
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "VerifyKeyConfig"
   },
   "source": [
    "## Step 3.2: Verify Key Configuration\n",
    "\n",
    "Check that the public key was assigned correctly by looking for the `RSA_PUBLIC_KEY_FP` (fingerprint) property."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___VerifyKeyConfig"
   },
   "outputs": [],
   "source": "DESC USER KAFKA_USER ->> SELECT * FROM $1 WHERE \"property\" = 'RSA_PUBLIC_KEY';",
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Part4_CreateInteractiveTable"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 4: Create Interactive Table\n",
    "\n",
    "Interactive Tables are a special type of Snowflake table optimized for low-latency, interactive workloads.\n",
    "\n",
    "## Key Characteristics of Interactive Tables\n",
    "- **Requires `CLUSTER BY`** - Choose columns used in your most frequent WHERE clauses\n",
    "- **Optimized for selective queries** - Best for queries that filter on clustered columns\n",
    "- **Works with Interactive Warehouses** - Must be added to an Interactive Warehouse for low-latency queries\n",
    "- **Supports streaming ingestion** - Works with Snowpipe Streaming v2\n",
    "\n",
    "For our IoT sensor data, we'll cluster by `device_id` and `timestamp` since those will be common filter conditions."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___CreateInteractiveTable"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE INTERACTIVE TABLE SENSOR_DATA (\n",
    "    RECORD_METADATA VARIANT,\n",
    "    RECORD_CONTENT VARIANT,\n",
    "    device_id VARCHAR(50),\n",
    "    sensor_type VARCHAR(50),\n",
    "    value FLOAT,\n",
    "    unit VARCHAR(20),\n",
    "    timestamp TIMESTAMP_NTZ,\n",
    "    location VARIANT\n",
    ")\n",
    "CLUSTER BY (device_id, timestamp);"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "VerifyTableCreation"
   },
   "source": [
    "### Verify Interactive Table Creation"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___VerifyTableCreation"
   },
   "outputs": [],
   "source": [
    "SHOW TABLES LIKE 'SENSOR_DATA';"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___GetDDL"
   },
   "outputs": [],
   "source": [
    "SELECT GET_DDL('TABLE', 'SENSOR_DATA');"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Part5_CreateInteractiveWarehouse",
    "collapsed": false
   },
   "source": "---\n\n# Part 5: Create Interactive Warehouse\n\nInteractive Warehouses are optimized for low-latency queries on Interactive Tables.\n\n## Key Characteristics of Interactive Warehouses\n- **Always running** - :red[They don't auto-suspend (you can manually suspend)]\n- **Low-latency optimized** - Tuned for sub-second query response\n- **Query timeout** - SELECT commands default to 5-second timeout\n- **Table restriction** - Can ONLY query Interactive Tables\n\nWe'll create an XSMALL warehouse which is appropriate for:\n- Working data sets less than 500 GB\n- Development and testing scenarios\n\n‚ö†Ô∏è _You will get a `NotebookSqlException` telling you that the `SENSOR_IWH` warehouse is suspended. This is okay. We are going to resume it next_.",
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___CreateInteractiveWarehouse"
   },
   "outputs": [],
   "source": "CREATE INTERACTIVE WAREHOUSE IF NOT EXISTS SENSOR_IWH\n    WAREHOUSE_SIZE = 'XSMALL';",
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "ResumeWarehouse",
    "collapsed": false
   },
   "source": [
    "### Resume the Interactive Warehouse\n",
    "\n",
    "Interactive Warehouses are created in a suspended state. We need to resume it before use."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___ResumeWarehouse"
   },
   "outputs": [],
   "source": [
    "ALTER WAREHOUSE SENSOR_IWH RESUME;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "VerifyWarehouse",
    "collapsed": false
   },
   "source": "### Verify Interactive Warehouse\n\nNote the `type` column in the response below for the `SENSOR_IWH` warehouse - it shows `INTERACTIVE` for \nthis Interactive Warehouse. Standard warehouses have a `type` of `STANDARD`.",
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___VerifyWarehouse"
   },
   "outputs": [],
   "source": [
    "SHOW WAREHOUSES LIKE 'SENSOR_IWH';"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000021"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "AddTableToWarehouse"
   },
   "source": [
    "### Add Interactive Table to Interactive Warehouse\n",
    "\n",
    "Before querying an Interactive Table from an Interactive Warehouse, you must explicitly add the table to the warehouse. This starts the cache-warming process."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___AddTableToWarehouse"
   },
   "outputs": [],
   "source": [
    "ALTER WAREHOUSE SENSOR_IWH ADD TABLES (SENSOR_DATA);"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000023"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Part6_GrantConnectorPerms",
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "# Part 6: Grant Permissions to Kafka Connector\n",
    "\n",
    "The Kafka connector needs ownership of the Interactive Table to stream data into it."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000024"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___GrantConnectorPerms"
   },
   "outputs": [],
   "source": [
    "GRANT INSERT, SELECT ON TABLE SENSOR_DATA TO ROLE KAFKA_CONNECTOR_ROLE;\n",
    "GRANT OWNERSHIP ON TABLE SENSOR_DATA TO ROLE KAFKA_CONNECTOR_ROLE REVOKE CURRENT GRANTS;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000025"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Part7_TestSetup"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 7: Test Setup (Before Kafka)\n",
    "\n",
    "Let's verify everything is set up correctly before starting the Kafka connector."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000026"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___TestSetup"
   },
   "outputs": [],
   "source": [
    "-- Switch to Interactive Warehouse for testing\n",
    "USE WAREHOUSE SENSOR_IWH;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000027"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___TestSetup2"
   },
   "outputs": [],
   "source": [
    "-- Verify table is accessible (will return 0 rows before streaming starts)\n",
    "SELECT COUNT(*) as total_records FROM SENSOR_DATA;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000028"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "SetupComplete",
    "collapsed": false
   },
   "source": "### Setup Complete! ‚úÖ\n\nThe Snowflake setup is now complete. The next step is to set up Kafka and Kafka Connect. Return to the \n[Quickstart](https://www.snowflake.com/en/developers/guides/kafka-interactive-tables-streaming/) for the steps, \nstarting at the **Setup Local Kafka Environment** section.",
   "id": "ce110000-1111-2222-3333-ffffff000029"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Part8_QueryData"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 8: Querying Streaming Data\n",
    "\n",
    "After you've configured and started the Kafka connector, run the cells below to query the streaming data.\n",
    "\n",
    "**Note:** Make sure you're using the Interactive Warehouse (`SENSOR_IWH`) for low-latency queries."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000030"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___UseWarehouse"
   },
   "outputs": [],
   "source": [
    "USE WAREHOUSE SENSOR_IWH;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000031"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "VerifyTestMessage"
   },
   "source": [
    "## Verify Test Message\n",
    "\n",
    "After sending your first test message with `send_message.py`, run this cell to verify it arrived in Snowflake.\n",
    "\n",
    "**Note:** It may take 5-10 seconds for data to appear due to Snowpipe Streaming latency."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000032"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___VerifyTestMessage"
   },
   "outputs": [],
   "source": [
    "-- Check all data in the table (useful for verifying first test message)\n",
    "SELECT * FROM SENSOR_DATA;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000033"
  },
  {
   "cell_type": "markdown",
   "id": "a7400b4e-ebc5-438a-9165-2d980b7dfd03",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## Start the Data Generator\n\nAt this point, let's start the data generator and get data flowing into our Interactive Table.\n\nReturn to the [Quickstart](https://www.snowflake.com/en/developers/guides/kafka-interactive-tables-streaming/) in the section \"Generate Continuous Data\"."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "SampleQueries",
    "collapsed": false
   },
   "source": "## Sample Queries\n\nBelow is a selection of a few sample queries we can issue against the streaming data, for illustrative purposes.\n\n### Recent Sensor Readings\n\nGet the most recent sensor readings from all devices.",
   "id": "ce110000-1111-2222-3333-ffffff000034"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___RecentSensor"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    device_id,\n",
    "    sensor_type,\n",
    "    value,\n",
    "    unit,\n",
    "    timestamp,\n",
    "    location:building::STRING as building,\n",
    "    location:floor::INTEGER as floor,\n",
    "    location:zone::STRING as zone\n",
    "FROM SENSOR_DATA\n",
    "WHERE timestamp >= DATEADD(minute, -5, CURRENT_TIMESTAMP())\n",
    "ORDER BY timestamp DESC\n",
    "LIMIT 20;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000035"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Aggregations",
    "collapsed": false
   },
   "source": "### Real-Time Aggregations\n\nCalculate average sensor values by type in the last minute.",
   "id": "ce110000-1111-2222-3333-ffffff000036"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___RealTimeAggregations"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    sensor_type,\n",
    "    COUNT(*) as reading_count,\n",
    "    ROUND(AVG(value), 2) as avg_value,\n",
    "    ROUND(MIN(value), 2) as min_value,\n",
    "    ROUND(MAX(value), 2) as max_value,\n",
    "    ROUND(STDDEV(value), 2) as stddev_value\n",
    "FROM SENSOR_DATA\n",
    "WHERE timestamp >= DATEADD(minute, -1, CURRENT_TIMESTAMP())\n",
    "GROUP BY sensor_type\n",
    "ORDER BY reading_count DESC;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000037"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "DeviceActivity",
    "collapsed": false
   },
   "source": "### Device Activity Monitor\n\nSee which devices are actively reporting in the last 30 seconds.",
   "id": "ce110000-1111-2222-3333-ffffff000038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___DeviceActivity"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    device_id,\n",
    "    COUNT(*) as readings,\n",
    "    MAX(timestamp) as last_reading,\n",
    "    DATEDIFF('second', MAX(timestamp), CURRENT_TIMESTAMP()) as seconds_ago\n",
    "FROM SENSOR_DATA\n",
    "WHERE timestamp >= DATEADD(second, -30, CURRENT_TIMESTAMP())\n",
    "GROUP BY device_id\n",
    "ORDER BY readings DESC;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000039"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "BuildingSummary",
    "collapsed": false
   },
   "source": "### Building Summary\n\nAggregate sensor data by building location.",
   "id": "ce110000-1111-2222-3333-ffffff000040"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___BuildingSummary"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    location:building::STRING as building,\n",
    "    COUNT(*) as total_readings,\n",
    "    COUNT(DISTINCT device_id) as active_devices,\n",
    "    COUNT(DISTINCT sensor_type) as sensor_types\n",
    "FROM SENSOR_DATA\n",
    "WHERE timestamp >= DATEADD(minute, -5, CURRENT_TIMESTAMP())\n",
    "GROUP BY location:building\n",
    "ORDER BY total_readings DESC;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000041"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "DataFreshness"
   },
   "source": [
    "## Data Freshness Check\n",
    "\n",
    "Verify the streaming pipeline is working by checking the most recent data."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000042"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___DataFreshness"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    COUNT(*) as total_records,\n",
    "    MIN(timestamp) as oldest_record,\n",
    "    MAX(timestamp) as newest_record,\n",
    "    DATEDIFF('second', MAX(timestamp), CURRENT_TIMESTAMP()) as data_lag_seconds\n",
    "FROM SENSOR_DATA;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000043"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Part9_Monitoring"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 9: Monitoring the Streaming Pipeline"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000044"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "ViewStreamingChannels"
   },
   "source": [
    "## View Streaming Channels\n",
    "\n",
    "Snowpipe Streaming v2 creates channels for data ingestion."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000045"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___ViewStreamingChannels"
   },
   "outputs": [],
   "source": [
    "SHOW CHANNELS IN SCHEMA KAFKA_INTERACTIVE.STREAMING;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000046"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "ViewDefaultPipe"
   },
   "source": [
    "## View Default Pipe\n",
    "\n",
    "Snowpipe Streaming v2 automatically creates a default pipe for each table."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000047"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___ViewDefaultPipe"
   },
   "outputs": [],
   "source": [
    "SHOW PIPES IN SCHEMA KAFKA_INTERACTIVE.STREAMING;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000048"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "TableStorageInfo"
   },
   "source": [
    "## Table Storage Information"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000049"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___TableStorageInfo"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    TABLE_NAME,\n",
    "    ROW_COUNT,\n",
    "    BYTES / (1024*1024) as SIZE_MB,\n",
    "    LAST_ALTERED\n",
    "FROM INFORMATION_SCHEMA.TABLES\n",
    "WHERE TABLE_NAME = 'SENSOR_DATA';"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000050"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Part10_Cleanup",
    "collapsed": false
   },
   "source": "---\n\n# Part 10: Cleanup\n\nRun these cells when you're done with the demo to clean up all Snowflake resources.\n\n:orange[Since the Interactive Warehouse does not auto-suspend, make sure to explicitly suspend it even\nif you do not remove the other SNowflake resources.]\n\n**‚ö†Ô∏è WARNING:** These commands will permanently delete all demo resources.\n\n**Before running cleanup:**\n1. Stop the data generator (Ctrl+C)\n2. Remove the Kafka connector: `curl -X DELETE http://localhost:8083/connectors/snowflake-sensor-data`\n3. Stop the Kafka cluster: `docker compose down -v`",
   "id": "ce110000-1111-2222-3333-ffffff000051"
  },
  {
   "cell_type": "code",
   "id": "1773598d-6e86-4a28-89d5-7e04cf4f36bf",
   "metadata": {
    "language": "sql",
    "name": "___SuspendWarehouse"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\n\n-- Suspend Interactive Warehouse\nALTER WAREHOUSE IF EXISTS SENSOR_IWH SUSPEND;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "___DropObjects"
   },
   "outputs": [],
   "source": "-- USE ROLE ACCOUNTADMIN;\n\n-- Drop Interactive Warehouse\n-- DROP WAREHOUSE IF EXISTS SENSOR_IWH;\n\n-- Drop table\n-- DROP TABLE IF EXISTS KAFKA_INTERACTIVE.STREAMING.SENSOR_DATA;\n\n-- Drop schema\n-- DROP SCHEMA IF EXISTS KAFKA_INTERACTIVE.STREAMING;\n\n-- Drop database\n-- DROP DATABASE IF EXISTS KAFKA_INTERACTIVE;\n\n-- Drop user and role\n-- DROP USER IF EXISTS KAFKA_USER;\n-- DROP ROLE IF EXISTS KAFKA_CONNECTOR_ROLE;",
   "id": "ce110000-1111-2222-3333-ffffff000052"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "CleanupComplete"
   },
   "source": [
    "### Cleanup Complete! üßπ\n",
    "\n",
    "Uncomment and run the commands above to remove all Snowflake resources."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000053"
  }
 ]
}