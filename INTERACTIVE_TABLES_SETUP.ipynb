{
 "metadata": {
  "kernelspec": {
   "display_name": "Snowflake",
   "language": "sql",
   "name": "snowflake"
  },
  "language_info": {
   "name": "sql"
  },
  "lastEditStatus": {
   "notebookId": "qrnf25scyeyjuh3aqdmi",
   "authorId": "432459825768",
   "authorName": "BMHESS",
   "authorEmail": "brian.hess@snowflake.com",
   "sessionId": "4996d791-81e0-49ec-814b-fbb1c83b09c5",
   "lastEditTime": 1768516690124
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "# Snowflake Setup for Kafka Interactive Tables Demo\n",
    "\n",
    "This notebook contains **all the SQL commands** needed to set up Snowflake for the Kafka Interactive Tables demo. Run this notebook before starting the Kafka connector.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Set Context** - Creates database and schema\n",
    "2. **Create Role and User** - Sets up Kafka connector authentication\n",
    "3. **Key Pair Authentication** - Configures RSA public key\n",
    "4. **Interactive Table Creation** - Creates an Interactive Table optimized for sensor data\n",
    "5. **Interactive Warehouse Setup** - Creates and configures an Interactive Warehouse\n",
    "6. **Grant Permissions** - Grants ownership to Kafka connector role\n",
    "7. **Sample Queries** - Provides queries for analyzing streaming data\n",
    "8. **Cleanup** - Commands to remove all demo resources\n",
    "\n",
    "## Prerequisites\n",
    "- ACCOUNTADMIN role access\n",
    "- RSA key pair generated (see instructions below)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 1: Set Context\n",
    "\n",
    "Create the database and schema for the demo."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\n\n-- Create database and schema if they don't exist\nCREATE DATABASE IF NOT EXISTS KAFKA_INTERACTIVE;\nCREATE SCHEMA IF NOT EXISTS KAFKA_INTERACTIVE.STREAMING;",
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "id": "eefc7e35-7962-4514-98a5-6cd6c1cef282",
   "metadata": {
    "language": "sql",
    "name": "cell56"
   },
   "outputs": [],
   "source": "-- Set context\nUSE DATABASE KAFKA_INTERACTIVE;\nUSE SCHEMA STREAMING;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell4"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 2: Create Role and User for Kafka Connector\n",
    "\n",
    "The Kafka connector needs its own role with specific permissions to:\n",
    "- Use the database and schema\n",
    "- Create and manage tables\n",
    "- Create stages and pipes for streaming"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "-- Create a role for the Kafka connector\nCREATE ROLE IF NOT EXISTS KAFKA_CONNECTOR_ROLE;\nGRANT ROLE KAFKA_CONNECTOR_ROLE TO ROLE ACCOUNTADMIN;\n\n-- Grant necessary privileges\nGRANT USAGE ON DATABASE KAFKA_INTERACTIVE TO ROLE KAFKA_CONNECTOR_ROLE;\nGRANT USAGE ON SCHEMA KAFKA_INTERACTIVE.STREAMING TO ROLE KAFKA_CONNECTOR_ROLE;\nGRANT CREATE TABLE ON SCHEMA KAFKA_INTERACTIVE.STREAMING TO ROLE KAFKA_CONNECTOR_ROLE;\nGRANT CREATE STAGE ON SCHEMA KAFKA_INTERACTIVE.STREAMING TO ROLE KAFKA_CONNECTOR_ROLE;\nGRANT CREATE PIPE ON SCHEMA KAFKA_INTERACTIVE.STREAMING TO ROLE KAFKA_CONNECTOR_ROLE;",
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "-- Create a user for the Kafka connector\n",
    "CREATE USER IF NOT EXISTS KAFKA_USER\n",
    "  DEFAULT_ROLE = KAFKA_CONNECTOR_ROLE\n",
    "  DEFAULT_NAMESPACE = KAFKA_INTERACTIVE.STREAMING;\n",
    "\n",
    "GRANT ROLE KAFKA_CONNECTOR_ROLE TO USER KAFKA_USER;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell7"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 3: Configure Key Pair Authentication\n",
    "\n",
    "The Kafka connector uses RSA key pair authentication for secure access to Snowflake.\n",
    "\n",
    "## Generate RSA Key Pair (Run in Terminal)\n",
    "\n",
    "Before running the next cell, generate an RSA key pair in your terminal:\n",
    "\n",
    "```bash\n",
    "# Generate private key (unencrypted for simplicity in this demo)\n",
    "openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt\n",
    "\n",
    "# Generate public key\n",
    "openssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub\n",
    "\n",
    "# Display the public key (you'll need this for the next step)\n",
    "cat rsa_key.pub\n",
    "```\n",
    "\n",
    "Copy the public key content (without the `-----BEGIN PUBLIC KEY-----` and `-----END PUBLIC KEY-----` lines)."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell8"
   },
   "source": [
    "## Step 3.1: Assign RSA Public Key to User\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:** Replace the placeholder below with your actual public key content (on a single line, no headers/footers)."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell9"
   },
   "outputs": [],
   "source": "-- Replace YOUR_PUBLIC_KEY_HERE with your actual public key content\n-- Example: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8A...\nALTER USER KAFKA_USER SET RSA_PUBLIC_KEY='YOUR_PUBLIC_KEY_HERE';",
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "## Step 3.2: Verify Key Configuration\n",
    "\n",
    "Check that the public key was assigned correctly by looking for the `RSA_PUBLIC_KEY_FP` (fingerprint) property."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": "DESC USER KAFKA_USER ->> SELECT * FROM $1 WHERE \"property\" = 'RSA_PUBLIC_KEY';",
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell12"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 4: Create Interactive Table\n",
    "\n",
    "Interactive Tables are a special type of Snowflake table optimized for low-latency, interactive workloads.\n",
    "\n",
    "## Key Characteristics of Interactive Tables\n",
    "- **Requires `CLUSTER BY`** - Choose columns used in your most frequent WHERE clauses\n",
    "- **Optimized for selective queries** - Best for queries that filter on clustered columns\n",
    "- **Works with Interactive Warehouses** - Must be added to an Interactive Warehouse for low-latency queries\n",
    "- **Supports streaming ingestion** - Works with Snowpipe Streaming v2\n",
    "\n",
    "For our IoT sensor data, we'll cluster by `device_id` and `timestamp` since those will be common filter conditions."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE INTERACTIVE TABLE SENSOR_DATA (\n",
    "    RECORD_METADATA VARIANT,\n",
    "    RECORD_CONTENT VARIANT,\n",
    "    device_id VARCHAR(50),\n",
    "    sensor_type VARCHAR(50),\n",
    "    value FLOAT,\n",
    "    unit VARCHAR(20),\n",
    "    timestamp TIMESTAMP_NTZ,\n",
    "    location VARIANT\n",
    ")\n",
    "CLUSTER BY (device_id, timestamp);"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell14"
   },
   "source": [
    "### Verify Interactive Table Creation"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "SHOW TABLES LIKE 'SENSOR_DATA';"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "SELECT GET_DDL('TABLE', 'SENSOR_DATA');"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": "---\n\n# Part 5: Create Interactive Warehouse\n\nInteractive Warehouses are optimized for low-latency queries on Interactive Tables.\n\n## Key Characteristics of Interactive Warehouses\n- **Always running** - They don't auto-suspend (you can manually suspend)\n- **Low-latency optimized** - Tuned for sub-second query response\n- **Query timeout** - SELECT commands default to 5-second timeout\n- **Table restriction** - Can ONLY query Interactive Tables\n\nWe'll create an XSMALL warehouse which is appropriate for:\n- Working data sets less than 500 GB\n- Development and testing scenarios\n\n‚ö†Ô∏è _You will get a `NotebookSqlException` telling you that the `SENSOR_IWH` warehouse is suspended. This is okay. We are going to resume it next_.",
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell18"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE INTERACTIVE WAREHOUSE SENSOR_IWH\n    WAREHOUSE_SIZE = 'XSMALL';",
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": [
    "### Resume the Interactive Warehouse\n",
    "\n",
    "Interactive Warehouses are created in a suspended state. We need to resume it before use."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "ALTER WAREHOUSE SENSOR_IWH RESUME;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell21"
   },
   "source": [
    "### Verify Interactive Warehouse"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "SHOW WAREHOUSES LIKE 'SENSOR_IWH';"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000021"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell23"
   },
   "source": [
    "### Add Interactive Table to Interactive Warehouse\n",
    "\n",
    "Before querying an Interactive Table from an Interactive Warehouse, you must explicitly add the table to the warehouse. This starts the cache-warming process."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "ALTER WAREHOUSE SENSOR_IWH ADD TABLES (SENSOR_DATA);"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000023"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell25"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 6: Grant Permissions to Kafka Connector\n",
    "\n",
    "The Kafka connector needs ownership of the Interactive Table to stream data into it."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000024"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell26"
   },
   "outputs": [],
   "source": [
    "GRANT INSERT, SELECT ON TABLE SENSOR_DATA TO ROLE KAFKA_CONNECTOR_ROLE;\n",
    "GRANT OWNERSHIP ON TABLE SENSOR_DATA TO ROLE KAFKA_CONNECTOR_ROLE REVOKE CURRENT GRANTS;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000025"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell27"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 7: Test Setup (Before Kafka)\n",
    "\n",
    "Let's verify everything is set up correctly before starting the Kafka connector."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000026"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell28"
   },
   "outputs": [],
   "source": [
    "-- Switch to Interactive Warehouse for testing\n",
    "USE WAREHOUSE SENSOR_IWH;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000027"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell29"
   },
   "outputs": [],
   "source": [
    "-- Verify table is accessible (will return 0 rows before streaming starts)\n",
    "SELECT COUNT(*) as total_records FROM SENSOR_DATA;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000028"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell30"
   },
   "source": [
    "### Setup Complete! ‚úÖ\n",
    "\n",
    "The Snowflake setup is now complete. Next steps:\n",
    "\n",
    "1. **Start Kafka** - Run `docker compose up -d` in your project directory\n",
    "2. **Create Kafka Topic** - Create the `sensor_data` topic\n",
    "3. **Configure Kafka Connector** - Deploy the Snowflake connector with your credentials\n",
    "4. **Start Streaming Data** - Run the data generator script\n",
    "5. **Query Data** - Use the cells below to query streaming data"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000029"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell31"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 8: Querying Streaming Data\n",
    "\n",
    "After you've configured and started the Kafka connector, run the cells below to query the streaming data.\n",
    "\n",
    "**Note:** Make sure you're using the Interactive Warehouse (`SENSOR_IWH`) for low-latency queries."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000030"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell32"
   },
   "outputs": [],
   "source": [
    "USE WAREHOUSE SENSOR_IWH;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000031"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell33"
   },
   "source": [
    "## Verify Test Message\n",
    "\n",
    "After sending your first test message with `send_message.py`, run this cell to verify it arrived in Snowflake.\n",
    "\n",
    "**Note:** It may take 5-10 seconds for data to appear due to Snowpipe Streaming latency."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000032"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell34"
   },
   "outputs": [],
   "source": [
    "-- Check all data in the table (useful for verifying first test message)\n",
    "SELECT * FROM SENSOR_DATA;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000033"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell35",
    "collapsed": false
   },
   "source": "## Sample Queries\n\nBelow is a selection of a few sample queries we can issue against the streaming data, for illustrative purposes.\n\n### Recent Sensor Readings\n\nGet the most recent sensor readings from all devices.",
   "id": "ce110000-1111-2222-3333-ffffff000034"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell36"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    device_id,\n",
    "    sensor_type,\n",
    "    value,\n",
    "    unit,\n",
    "    timestamp,\n",
    "    location:building::STRING as building,\n",
    "    location:floor::INTEGER as floor,\n",
    "    location:zone::STRING as zone\n",
    "FROM SENSOR_DATA\n",
    "WHERE timestamp >= DATEADD(minute, -5, CURRENT_TIMESTAMP())\n",
    "ORDER BY timestamp DESC\n",
    "LIMIT 20;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000035"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell37",
    "collapsed": false
   },
   "source": "### Real-Time Aggregations\n\nCalculate average sensor values by type in the last minute.",
   "id": "ce110000-1111-2222-3333-ffffff000036"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell38"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    sensor_type,\n",
    "    COUNT(*) as reading_count,\n",
    "    ROUND(AVG(value), 2) as avg_value,\n",
    "    ROUND(MIN(value), 2) as min_value,\n",
    "    ROUND(MAX(value), 2) as max_value,\n",
    "    ROUND(STDDEV(value), 2) as stddev_value\n",
    "FROM SENSOR_DATA\n",
    "WHERE timestamp >= DATEADD(minute, -1, CURRENT_TIMESTAMP())\n",
    "GROUP BY sensor_type\n",
    "ORDER BY reading_count DESC;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000037"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell39",
    "collapsed": false
   },
   "source": "### Device Activity Monitor\n\nSee which devices are actively reporting in the last 30 seconds.",
   "id": "ce110000-1111-2222-3333-ffffff000038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell40"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    device_id,\n",
    "    COUNT(*) as readings,\n",
    "    MAX(timestamp) as last_reading,\n",
    "    DATEDIFF('second', MAX(timestamp), CURRENT_TIMESTAMP()) as seconds_ago\n",
    "FROM SENSOR_DATA\n",
    "WHERE timestamp >= DATEADD(second, -30, CURRENT_TIMESTAMP())\n",
    "GROUP BY device_id\n",
    "ORDER BY readings DESC;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000039"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell41",
    "collapsed": false
   },
   "source": "### Building Summary\n\nAggregate sensor data by building location.",
   "id": "ce110000-1111-2222-3333-ffffff000040"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell42"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    location:building::STRING as building,\n",
    "    COUNT(*) as total_readings,\n",
    "    COUNT(DISTINCT device_id) as active_devices,\n",
    "    COUNT(DISTINCT sensor_type) as sensor_types\n",
    "FROM SENSOR_DATA\n",
    "WHERE timestamp >= DATEADD(minute, -5, CURRENT_TIMESTAMP())\n",
    "GROUP BY location:building\n",
    "ORDER BY total_readings DESC;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000041"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell43"
   },
   "source": [
    "## Data Freshness Check\n",
    "\n",
    "Verify the streaming pipeline is working by checking the most recent data."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000042"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell44"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    COUNT(*) as total_records,\n",
    "    MIN(timestamp) as oldest_record,\n",
    "    MAX(timestamp) as newest_record,\n",
    "    DATEDIFF('second', MAX(timestamp), CURRENT_TIMESTAMP()) as data_lag_seconds\n",
    "FROM SENSOR_DATA;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000043"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell45"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 9: Monitoring the Streaming Pipeline"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000044"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell46"
   },
   "source": [
    "## View Streaming Channels\n",
    "\n",
    "Snowpipe Streaming v2 creates channels for data ingestion."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000045"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell47"
   },
   "outputs": [],
   "source": [
    "SHOW CHANNELS IN SCHEMA KAFKA_INTERACTIVE.STREAMING;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000046"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell48"
   },
   "source": [
    "## View Default Pipe\n",
    "\n",
    "Snowpipe Streaming v2 automatically creates a default pipe for each table."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000047"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell49"
   },
   "outputs": [],
   "source": [
    "SHOW PIPES IN SCHEMA KAFKA_INTERACTIVE.STREAMING;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000048"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell50"
   },
   "source": [
    "## Table Storage Information"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000049"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell51"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    TABLE_NAME,\n",
    "    ROW_COUNT,\n",
    "    BYTES / (1024*1024) as SIZE_MB,\n",
    "    LAST_ALTERED\n",
    "FROM INFORMATION_SCHEMA.TABLES\n",
    "WHERE TABLE_NAME = 'SENSOR_DATA';"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000050"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell52"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 10: Cleanup\n",
    "\n",
    "Run these cells when you're done with the demo to clean up all Snowflake resources.\n",
    "\n",
    "**‚ö†Ô∏è WARNING:** These commands will permanently delete all demo resources.\n",
    "\n",
    "**Before running cleanup:**\n",
    "1. Stop the data generator (Ctrl+C)\n",
    "2. Remove the Kafka connector: `curl -X DELETE http://localhost:8083/connectors/snowflake-sensor-data`\n",
    "3. Stop the Kafka cluster: `docker compose down -v`"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000051"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell53"
   },
   "outputs": [],
   "source": [
    "-- USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "-- Suspend and drop Interactive Warehouse\n",
    "-- ALTER WAREHOUSE IF EXISTS SENSOR_IWH SUSPEND;\n",
    "-- DROP WAREHOUSE IF EXISTS SENSOR_IWH;\n",
    "\n",
    "-- Drop table\n",
    "-- DROP TABLE IF EXISTS KAFKA_INTERACTIVE.STREAMING.SENSOR_DATA;\n",
    "\n",
    "-- Drop schema\n",
    "-- DROP SCHEMA IF EXISTS KAFKA_INTERACTIVE.STREAMING;\n",
    "\n",
    "-- Drop database\n",
    "-- DROP DATABASE IF EXISTS KAFKA_INTERACTIVE;\n",
    "\n",
    "-- Drop user and role\n",
    "-- DROP USER IF EXISTS KAFKA_USER;\n",
    "-- DROP ROLE IF EXISTS KAFKA_CONNECTOR_ROLE;"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000052"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell54"
   },
   "source": [
    "### Cleanup Complete! üßπ\n",
    "\n",
    "Uncomment and run the commands above to remove all Snowflake resources."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000053"
  }
 ]
}